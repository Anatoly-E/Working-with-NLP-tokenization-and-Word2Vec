"""
–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã –ø–æ NLP
"""

import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem import SnowballStemmer  # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Å—Ç–µ–º–º–∏–Ω–≥–∞
import pymorphy3  # –î–ª—è —Ä—É—Å—Å–∫–æ–π –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏

class TextPreprocessor:
    def __init__(self, language='russian'):
        self.language = language

        if language == 'russian':
            # –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            self.stop_words = set(stopwords.words('russian'))
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            russian_stopwords_extended = [
                '—ç—Ç–æ', '–∫–∞–∫', '—Ç–∞–∫', '–∏', '–≤', '–Ω–∞–¥', '–∫', '–¥–æ', '–Ω–µ', '–Ω–∞', '–Ω–æ', '–∑–∞', 
                '—Ç–æ', '—Å', '–ª–∏', '–∞', '–≤–æ', '–æ—Ç', '—Å–æ', '–¥–ª—è', '–æ', '–∂–µ', '–Ω—É', '–≤—ã', 
                '–±—ã', '—á—Ç–æ', '–∫—Ç–æ', '–æ–Ω', '–æ–Ω–∞'
            ]
            self.stop_words.update(russian_stopwords_extended)
            
            # –†—É—Å—Å–∫–∞—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è
            self.punctuation = set(string.punctuation + '¬´¬ª‚Äî‚Ä¶')
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
            try:
                self.morph = pymorphy3.MorphAnalyzer()
                print("‚úÖ PyMorphy3 –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è —Ä—É—Å—Å–∫–æ–π –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏")
            except ImportError:
                print("‚ùå PyMorphy3 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pymorphy3")
                self.morph = None
                
            self.stemmer = SnowballStemmer('russian')
            
        else:  # english
            self.stop_words = set(stopwords.words('english'))
            self.punctuation = set(string.punctuation)
            self.lemmatizer = WordNetLemmatizer()
            self.morph = None
            self.stemmer = None

    def preprocess_text(self, text):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
        print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞...")
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        tokens = word_tokenize(text.lower())
        print(f"üìù –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å–ª–æ–≤: {len(tokens):,}")
        
        # –û—á–∏—Å—Ç–∫–∞
        cleaned_tokens = self._clean_tokens(tokens)
        
        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è/—Å—Ç–µ–º–º–∏–Ω–≥
        if self.language == 'russian' and self.morph:
            cleaned_tokens = self._lemmatize_russian(cleaned_tokens)
        elif self.language == 'english':
            cleaned_tokens = self._lemmatize_english(cleaned_tokens)
        
        print(f"‚ú® –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å: {len(cleaned_tokens):,} —Ç–æ–∫–µ–Ω–æ–≤")
        return cleaned_tokens
    
    def _clean_tokens(self, tokens):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏"""
        cleaned = []
        for token in tokens:
            if (token not in self.stop_words and 
                token not in self.punctuation and
                token.isalpha() and 
                len(token) > 2):
                cleaned.append(token)
        return cleaned
    
    def _lemmatize_russian(self, tokens):
        """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Å –ø–æ–º–æ—â—å—é pymorphy3"""
        print("üî§ –õ–µ–º–º–∞—Ç–∏–∑–∏—Ä—É–µ–º —Ä—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞...")
        lemmatized = []
        for token in tokens:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É —Å–ª–æ–≤–∞
                parsed = self.morph.parse(token)[0]
                lemma = parsed.normal_form
                lemmatized.append(lemma)
            except Exception as e:
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ
                lemmatized.append(token)
        return lemmatized
    
    def _lemmatize_english(self, tokens):
        """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
        return [self.lemmatizer.lemmatize(token) for token in tokens]

    def _lemmatize_tokens(self, tokens):
        """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤"""
        return [self.lemmatizer.lemmatize(token) for token in tokens]
    
    def get_sample_tokens(self, tokens, n=20):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏"""
        return tokens[:n]