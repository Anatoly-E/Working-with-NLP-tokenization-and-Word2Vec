"""
–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã –ø–æ NLP
"""

import string
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

class TextPreprocessor:
    def __init__(self, language='english'):
        self.language = language
        self.stop_words = set(stopwords.words(language))
        self.punctuation = set(string.punctuation)
        self.lemmatizer = WordNetLemmatizer()
        
    def preprocess_text(self, text):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"""
        print("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞...")
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        tokens = word_tokenize(text.lower())
        print(f"üìù –¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å–ª–æ–≤: {len(tokens):,}")
        
        # –û—á–∏—Å—Ç–∫–∞
        cleaned_tokens = self._clean_tokens(tokens)
        
        # –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
        cleaned_tokens = self._lemmatize_tokens(cleaned_tokens)
        
        print(f"‚ú® –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å: {len(cleaned_tokens):,} —Ç–æ–∫–µ–Ω–æ–≤")
        return cleaned_tokens
    
    def _clean_tokens(self, tokens):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏"""
        cleaned = []
        for token in tokens:
            if (token not in self.stop_words and 
                token not in self.punctuation and
                token.isalpha() and 
                len(token) > 2):
                cleaned.append(token)
        return cleaned
    
    def _lemmatize_tokens(self, tokens):
        """–õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤"""
        return [self.lemmatizer.lemmatize(token) for token in tokens]
    
    def get_sample_tokens(self, tokens, n=20):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–∏–º–µ—Ä —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏"""
        return tokens[:n]