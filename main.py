"""
–ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã –ø–æ NLP
–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ Word2Vec
"""

import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø–∞–ø–∫—É src –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
current_dir = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.join(current_dir, 'src')
if src_path not in sys.path:
    sys.path.insert(0, src_path)

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ —Ç–æ—á–Ω–æ –µ—Å—Ç—å –≤ –º–æ–¥—É–ª—è—Ö
from config import LANGUAGE_CONFIGS, DATA_PATHS, CURRENT_LANGUAGE
from src.data_loader import download_nltk_resources, load_gutenberg_corpus, load_russian_corpus, load_custom_corpus, save_processed_data
from src.preprocessor import TextPreprocessor
from src.word2vec_train import Word2VecTrainer
from src.visualizer import EmbeddingVisualizer
    
def select_language():
    """–ü–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –≤—ã–±—Ä–∞—Ç—å —è–∑—ã–∫"""
    print("üåç –í–´–ë–ï–†–ò–¢–ï –Ø–ó–´–ö –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:")
    print("   1. –ê–Ω–≥–ª–∏–π—Å–∫–∏–π (Alice in Wonderland)")
    print("   2. –†—É—Å—Å–∫–∏–π (–ê–ª–∏—Å–∞ –≤ –°—Ç—Ä–∞–Ω–µ —á—É–¥–µ—Å)")
    
    while True:
        choice = input("\nüéØ –í–≤–µ–¥–∏—Ç–µ 1 –∏–ª–∏ 2: ").strip()
        if choice == '1':
            return 'english'
        elif choice == '2':
            return 'russian'
        else:
            print("‚ùå –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ 1 –∏–ª–∏ 2")

def get_language_config(language):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞"""
    return LANGUAGE_CONFIGS[language]

def main():
    print("üöÄ –ó–∞–ø—É—Å–∫ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç—ã: NLP —Å Word2Vec")
    print("=" * 50)
    
    # –í—ã–±–æ—Ä —è–∑—ã–∫–∞
    language = select_language()
    config = get_language_config(language)
    
    print(f"\nüåç –í—ã–±—Ä–∞–Ω —è–∑—ã–∫: {language.upper()}")
    print("=" * 50)
    
    # –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüì• –®–ê–ì 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    download_nltk_resources()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —è–∑—ã–∫–∞
    if language == 'english':
        text = load_gutenberg_corpus(config['book_name'])
        corpus_type = "Gutenberg (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π)"
    else:  # russian
        text = load_russian_corpus(DATA_PATHS['russian_file_path'])
        corpus_type = "–†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç"
    
    if not text:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ!")
        return
    
    print(f"üìö –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: {corpus_type}")
    
    # –®–∞–≥ 2: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    print("\nüõ†Ô∏è –®–ê–ì 2: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞...")
    preprocessor = TextPreprocessor(language=config['preprocessing']['language'])
    processed_tokens = preprocessor.preprocess_text(text)
    
    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    print(f"\nüîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ ({language}):")
    from collections import Counter
    word_freq = Counter(processed_tokens)
    
    test_words = config['test_words']
    found_words = []
    
    for word in test_words:
        if word in word_freq:
            print(f"   ‚úÖ '{word}': –Ω–∞–π–¥–µ–Ω–æ ({word_freq[word]} —Ä–∞–∑)")
            found_words.append(word)
        else:
            print(f"   ‚ùå '{word}': –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    
    if len(found_words) < 3:
        print(f"‚ö†Ô∏è –ú–∞–ª–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–∞–π–¥–µ–Ω–æ. –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞.")
        found_words = [word for word, count in word_freq.most_common(10)]
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    filename = f"processed_tokens_{language}.txt"
    save_processed_data(processed_tokens, filename)
    
    # –®–∞–≥ 3: –û–±—É—á–µ–Ω–∏–µ Word2Vec
    print("\nüéØ –®–ê–ì 3: –û–±—É—á–µ–Ω–∏–µ Word2Vec –º–æ–¥–µ–ª–∏...")
    trainer = Word2VecTrainer(config['word2vec'])
    model = trainer.train_model(processed_tokens)
    
    # –ò—Å—Å–ª–µ–¥—É–µ–º –º–æ–¥–µ–ª—å
    print(f"\nüîç –ò—Å—Å–ª–µ–¥—É–µ–º –º–æ–¥–µ–ª—å ({language}):")
    trainer.explore_model(found_words)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    model_path = f'models/word2vec_model_{language}'
    trainer.save_model(model_path)
    
    # –®–∞–≥ 4: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    print("\nüé® –®–ê–ì 4: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    visualizer = EmbeddingVisualizer(config['visualization'], language=language)
    success = visualizer.plot_embeddings(model)
    
    if not success:
        print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é")
    
    print("\n" + "=" * 50)
    print(f"‚úÖ –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ! (–Ø–∑—ã–∫: {language})")
    print("üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–∞—Ö:")
    print(f"   - models/word2vec_model_{language}/word2vec.model")
    print(f"   - results/plots/word_embeddings.png")
    print(f"   - data/processed/processed_tokens_{language}.txt")

if __name__ == "__main__":
    main()